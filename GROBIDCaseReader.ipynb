{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beea7fc0-b1c0-4ff1-8787-0d6505109748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all Libraries - RUN THIS BEFORE ANY TESTING\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tkinter as tk\n",
    "import PyPDF2\n",
    "import re\n",
    "from tkinter import filedialog\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "879fa66b-adf5-43af-835d-e6e907ca069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter start page number:  1\n",
      "Enter end page number:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to/home/sean/Downloads/1RobertsvQueen3_EDITED.pdf\n"
     ]
    }
   ],
   "source": [
    "#This cell points the script to GROBID's API, allows the user to choose a PDF and specify a page range, then saves the edited PDF.\n",
    "\n",
    "#RUN GROBID FIRST USING ./gradlew run IN THE GROBID DIRECTORY\n",
    "\n",
    "# Define the URL for GROBID's PDF conversion endpoint\n",
    "grobid_url = \"http://localhost:8070/api/processFulltextDocument\"\n",
    "\n",
    "#Prompt user to select PDF file\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "pdf_file_path = filedialog.askopenfilename()\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "pdf_file = open(pdf_file_path, 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "# Prompt user to enter start and end page numbers\n",
    "start_page = int(input(\"Enter start page number: \"))\n",
    "end_page = int(input(\"Enter end page number: \"))\n",
    "\n",
    "# Limit end page to total number of pages in PDF\n",
    "if end_page > len(pdf_reader.pages):\n",
    "    end_page = len(pdf_reader.pages)\n",
    "    \n",
    "# Create a PDF writer object\n",
    "pdf_writer = PyPDF2.PdfWriter()\n",
    "\n",
    "# Loop over the selected pages and add them to the PDF writer\n",
    "for page_num in range(start_page-1, end_page):\n",
    "    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "\n",
    "# Save the output PDF file to the same directory as the original PDF\n",
    "output_file_path = os.path.splitext(pdf_file_path)[0] + '_EDITED.pdf'\n",
    "with open(output_file_path, 'wb') as f:\n",
    "    pdf_writer.write(f)\n",
    "    \n",
    "edited_pdf = output_file_path\n",
    "print(\"Saved to\" + edited_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e102ac42-b615-4d5b-ab87-1dc660a67362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell defines the parameters for GROBID's XML conversion, parses text through GROBID and outputs it as a .xml file\n",
    "\n",
    "# Redefine pdf_file_path as the new, edited PDF file\n",
    "pdf_file_path = output_file_path\n",
    "    \n",
    "## Send the selected pages of the PDF file to GROBID for conversion to XML with sentence-level segmentation\n",
    "with open(pdf_file_path, 'rb') as f:\n",
    "    # Set the start and end page parameters in the API call, and specify both teiCoordinates and sentence-level segmentation\n",
    "    params = {\n",
    "        'start': start_page,\n",
    "        'end': end_page,\n",
    "        'segmentation': 'teiCoordinates,sentences',\n",
    "        'consolidate_citations': False,\n",
    "        'consolidate_header': False,\n",
    "        'tei': '''\n",
    "            <tei>\n",
    "                <teiHeader>\n",
    "                    <!-- Add any modifications to the header here -->\n",
    "                </teiHeader>\n",
    "                <text>\n",
    "                    <body>\n",
    "                        <div type=\"abstract\">\n",
    "                            {abstract}\n",
    "                        </div>\n",
    "                        <div type=\"main\">\n",
    "                            {main}\n",
    "                        </div>\n",
    "                    </body>\n",
    "                </text>\n",
    "            </tei>\n",
    "        ''',\n",
    "        'input': 'file',\n",
    "        'consolidateHeader': 1,\n",
    "        'consolidateCitations': 1,\n",
    "        'includeRawCitations': 0,\n",
    "        'teiCoordinates': 1,\n",
    "        'disableLinks': 1, # Add this parameter to disable footnote extraction\n",
    "        'disableFootnotes': 1, # Add this parameter to disable footnotes processing\n",
    "    }\n",
    "    files = {'input': f}\n",
    "    response = requests.post(grobid_url, params=params, files=files)\n",
    "\n",
    "# Save the GROBID XML response to the same file with an appended '_GROBID' suffix and a .xml extension\n",
    "grobid_filename = os.path.splitext(pdf_file_path)[0] + f'_{start_page}-{end_page}_GROBID.xml'\n",
    "with open(grobid_filename, 'w') as f:\n",
    "    f.write(response.text)\n",
    "    \n",
    "os.remove(edited_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb44967c-7b84-430a-94fb-93d89aed30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned .xml to:/home/sean/Downloads/1RobertsvQueen3_EDITED_1-26_GROBID2.xml\n"
     ]
    }
   ],
   "source": [
    "#This cell extracts the body of the .xml file, preserving all tags.\n",
    "\n",
    "# Read in the contents of the GROBID XML file\n",
    "with open(grobid_filename, 'r') as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Extract everything between the <body> and </body> tags\n",
    "body_contents = re.search('(<body>)(.*)(</body>)', file_contents, re.DOTALL).group(0)\n",
    "\n",
    "# Construct the new file name by appending \"2\" to the original file name\n",
    "new_file_name = os.path.splitext(grobid_filename)[0] + \"2.xml\"\n",
    "\n",
    "# Write the extracted contents to a new file with the new file name\n",
    "with open(new_file_name, 'w') as new_file:\n",
    "    new_file.write(body_contents)\n",
    "    \n",
    "cleaned_xml = new_file_name\n",
    "print(\"Saved cleaned .xml to:\" + cleaned_xml)\n",
    "\n",
    "#Removes the intermediary file\n",
    "os.remove(grobid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba772050-c978-4315-8e01-722a15970cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up/home/sean/Downloads/1RobertsvQueen3_EDITED_1-26_GROBID2_CLEANED.txt\n"
     ]
    }
   ],
   "source": [
    "#This cell formats the .xml file\n",
    "\n",
    "# Open the existing .xml file\n",
    "with open(cleaned_xml, 'r') as f:\n",
    "    xml_content = f.read()\n",
    "\n",
    "# Replace <s> and <p> tags, and add a line break to <head> tags in .xml\n",
    "soup = BeautifulSoup(xml_content, 'xml')\n",
    "for tag in soup.find_all(['p', 's']):\n",
    "    tag.replace_with(tag.text + ' ')\n",
    "for tag in soup.find_all('head'):\n",
    "    tag.replace_with('\\n' + tag.text + '\\n')\n",
    "cleaned_text = soup.get_text()\n",
    "\n",
    "# Remove HTML and CSS tags from the cleaned text\n",
    "soup = BeautifulSoup(cleaned_text, 'html.parser')\n",
    "cleaned_text = soup.get_text()\n",
    "\n",
    "# Remove HTML and CSS tags from the cleaned text\n",
    "soup = BeautifulSoup(cleaned_text, 'html.parser')\n",
    "for tag in soup.find_all(['style', 'script']):\n",
    "    tag.unwrap()\n",
    "cleaned_text = soup.prettify(formatter=None)\n",
    "\n",
    "# Save the final cleaned text in a .txt file with an appended '_CLEANED' suffix\n",
    "cleaned_filename = os.path.splitext(cleaned_xml)[0] + '_CLEANED.txt'\n",
    "with open(cleaned_filename, 'w') as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "#Removes the intermediary file, saves new variables for next cell\n",
    "os.remove(cleaned_xml)\n",
    "filename = cleaned_filename\n",
    "print(\"Cleaned up\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c36d554-76af-4bd5-a138-51588cd08796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sean/Downloads/Roberts v Queen.txt\n"
     ]
    }
   ],
   "source": [
    "#This cell uses regular expressions to clean up the text\n",
    "\n",
    "# Define the regular expressions to use for cleaning the text\n",
    "square_brackets_pattern = r'\\[[^\\[\\]a-zA-Z]{1,30}\\]'\n",
    "parentheses_pattern = r'\\([^()]{1,75}\\)'\n",
    "url_regex = re.compile(r'(?:https?://|www\\.)\\S+\\b')\n",
    "date_regex = re.compile(r'\\d{1,2}\\/\\d{1,2}\\/(?:\\d{4}|\\d{2})|\\d{4}-\\d{2}-\\d{2}')\n",
    "timestamp_regex = re.compile(r'\\b\\d{1,2}(?::\\d{2}){1,2}(?: ?[AP]M)?(?!\\w|\\.)\\b')\n",
    "doi_regex = re.compile(r'\\b10\\.\\d{4,9}/[-._;()/:A-Z0-9]+|\\bdoi:\\S+\\b|\\bdoi:\\S+\\b|\\bDOI:\\S+\\b')\n",
    "email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
    "grobid_regex = re.compile(r'GROBID - A machine learning software for extracting information from scholarly documents')\n",
    "uuid_regex = re.compile(r'\\b[0-9a-fA-F]{32}\\b')\n",
    "aglc_regex = re.compile(r'\\b[A-Z]{1,3}\\s[1-9]\\d{0,2}(?:\\([1-9]\\d{0,2}\\))?')\n",
    "\n",
    "# Add space before 2 or 3 digit numbers using regular expression\n",
    "cleaned_text = re.sub(r'(?<=\\D)(\\d{2,3})\\b', r' \\1', cleaned_text)\n",
    "\n",
    "# Define a regular expression pattern to match the end of a sentence\n",
    "sentence_end_regex = re.compile(r'([.!?])\\s+')\n",
    "\n",
    "# Read in the file and remove the matched text using the regular expressions\n",
    "with open(filename, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove Matched Text\n",
    "text = re.sub(square_brackets_pattern, '', text)\n",
    "text = re.sub(parentheses_pattern, '', text)\n",
    "text = aglc_regex.sub('', text)\n",
    "text = url_regex.sub('', text)\n",
    "text = date_regex.sub('', text)\n",
    "text = doi_regex.sub('', text)\n",
    "text = email_regex.sub('', text)\n",
    "text = grobid_regex.sub('', text)\n",
    "text = uuid_regex.sub('', text)\n",
    "\n",
    "#Use a regular expression pattern to add a line break after each sentence\n",
    "text = sentence_end_regex.sub(r'\\1\\n', text)\n",
    "\n",
    "# Ask the user to choose a file name and location to save the processed text\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "new_file_path = filedialog.asksaveasfilename(defaultextension='.txt')\n",
    "\n",
    "# Create a new file with the cleaned text\n",
    "with open(new_file_path, 'w') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Remove the original file\n",
    "os.remove(filename)\n",
    "\n",
    "filename = new_file_path\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f04ffbc3-ce66-4f63-9cdb-cad8856d87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell chooses a language, model and vocoder from Espnet2\n",
    "\n",
    "lang = 'English'\n",
    "tag = 'kan-bayashi/ljspeech_fastspeech2' #@param [\"kan-bayashi/ljspeech_tacotron2\", \"kan-bayashi/ljspeech_fastspeech\", \"kan-bayashi/ljspeech_fastspeech2\", \"kan-bayashi/ljspeech_conformer_fastspeech2\", \"kan-bayashi/ljspeech_joint_finetune_conformer_fastspeech2_hifigan\", \"kan-bayashi/ljspeech_joint_train_conformer_fastspeech2_hifigan\", \"kan-bayashi/ljspeech_vits\"] {type:\"string\"}\n",
    "vocoder_tag = \"parallel_wavegan/ljspeech_parallel_wavegan.v1\" #@param [\"none\", \"parallel_wavegan/ljspeech_parallel_wavegan.v1\", \"parallel_wavegan/ljspeech_full_band_melgan.v2\", \"parallel_wavegan/ljspeech_multi_band_melgan.v2\", \"parallel_wavegan/ljspeech_hifigan.v1\", \"parallel_wavegan/ljspeech_style_melgan.v1\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df087d90-447d-413d-8b63-bc971dfffe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell defines the parameters for Espnet2 text to speech\n",
    "\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "from espnet2.utils.types import str_or_none\n",
    "\n",
    "text2speech = Text2Speech.from_pretrained(\n",
    "    model_tag=str_or_none(tag),\n",
    "    vocoder_tag=str_or_none(vocoder_tag),\n",
    "    device=\"cuda\",\n",
    "    # Only for Tacotron 2 & Transformer\n",
    "    threshold=0.5,\n",
    "    # Only for Tacotron 2\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=10.0,\n",
    "    use_att_constraint=False,\n",
    "    backward_window=1,\n",
    "    forward_window=3,\n",
    "    # Only for FastSpeech & FastSpeech2 & VITS\n",
    "    speed_control_alpha=1.0,\n",
    "    # Only for VITS\n",
    "    noise_scale=0.333,\n",
    "    noise_scale_dur=0.333,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5946ad06-497c-47aa-994f-e837bea7a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/297\n",
      "Processing chunk 2/297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4415/1710974453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext2speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mout_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"out{i+1}{out_extension}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PCM_16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0msound_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_extension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This cell converts the cleaned and processed text into a text to speech reading, breaking the text into chunks to preserve VRAM, and converting the output to an .mp3\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Split the text into chunks of 256 characters\n",
    "chunk_size = 256\n",
    "\n",
    "# Get the file extension and use it to determine the output file extension\n",
    "file_extension = os.path.splitext(filename)[1]\n",
    "\n",
    "if file_extension == '.txt':\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    out_extension = '.wav'\n",
    "elif file_extension == '.wav':\n",
    "    chunks = [filename]\n",
    "    out_extension = '.wav'\n",
    "else:\n",
    "    print(\"Invalid file type.\")\n",
    "    exit()\n",
    "\n",
    "# synthesis\n",
    "sound_files = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "    with torch.no_grad():\n",
    "        wav = text2speech(chunk)[\"wav\"]\n",
    "        out_filename = f\"out{i+1}{out_extension}\"\n",
    "        sf.write(out_filename, wav.view(-1).cpu().numpy(), text2speech.fs, \"PCM_16\")\n",
    "        sound_files.append(AudioSegment.from_file(out_filename, out_extension.replace('.', '')))\n",
    "\n",
    "# Combine all the sound files\n",
    "combined_sounds = sound_files[0]\n",
    "for sound in sound_files[1:]:\n",
    "    combined_sounds += sound\n",
    "\n",
    "# Export the combined sound file as a .wav file\n",
    "out_filename = os.path.splitext(filename)[0] + out_extension\n",
    "combined_sounds.export(out_filename, format=out_extension.replace('.', ''))\n",
    "\n",
    "# Convert the output file from .wav to .mp3\n",
    "mp3_filename = os.path.splitext(filename)[0] + '.mp3'\n",
    "AudioSegment.from_wav(out_filename).export(mp3_filename, format='mp3')\n",
    "\n",
    "# Delete intermediate sound files\n",
    "for i in range(1, len(chunks)+1):\n",
    "    filename = f\"out{i}{out_extension}\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "# Delete the temporary .wav file\n",
    "if os.path.exists(out_filename):\n",
    "    os.remove(out_filename)\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7307d9-9cde-4ae5-801d-a6421748f554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
